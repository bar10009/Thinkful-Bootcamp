{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization parameter: Lasso\n",
    "The  λλ  for lasso can var between 0 (no penalty, acts like OLS) and infinity. If  λλ  is too large, all parameters will be set to zero.\n",
    "\n",
    "Create a plot below of how  R2R2  varies across different values of  λλ  for ridge and lasso regression. Use logic and code similar to the ridge regression demonstration above, and base your plot on the X_train2 feature set.\n",
    "\n",
    "Do lasso and ridge yield the same  R2R2  for a given lambda value?\n",
    "\n",
    "Submit your work and discuss the results with your mentor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lasso Regression and L1 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data again. Keep air quality data, drop the index column\n",
    "# and any missing data columns.\n",
    "df = pd.read_csv(\n",
    "    'https://vincentarelbundock.github.io/Rdatasets/csv/ISLR/Default.csv'\n",
    ").iloc[:,1:].dropna()\n",
    "\n",
    "# Recode strings to numeric.\n",
    "df['default'] = np.where(df['default']=='Yes', 1, 0)\n",
    "df['student'] = np.where(df['student']=='Yes', 1, 0)\n",
    "names = df.columns\n",
    "df = pd.DataFrame(preprocessing.scale(df), columns=names)\n",
    "\n",
    "# Define the training and test sizes.\n",
    "trainsize = int(df.shape[0] / 2)\n",
    "df_test = df.iloc[trainsize:, :].copy()\n",
    "df_train = df.iloc[:trainsize, :].copy()\n",
    "\n",
    "Y_train = df_train['income'].values.reshape(-1, 1)\n",
    "X_train = df_train.loc[:, ~(df_train.columns).isin(['income'])]\n",
    "\n",
    "# Make some new features to capture potential quadratic and cubic\n",
    "# relationships between solar radiation and day or temperature.\n",
    "df_train['balance_student'] = df_train['balance'] * df_train['student']\n",
    "df_train['balance_default'] = df_train['balance'] * df_train['default']\n",
    "df_train['student_default'] = df_train['student'] * df_train['default']\n",
    "df_train['balance_sqrt'] = (df_train['balance'] + 100) ** .5\n",
    "df_train['balance2'] = (df_train['balance'] + 100) ** 2\n",
    "df_train['balance3'] = (df_train['balance'] + 100) ** 3\n",
    "\n",
    "X_train2 = df_train.loc[:, ~(df_train.columns).isin(['income'])]\n",
    "\n",
    "# Test the simpler model with smaller coefficients.\n",
    "Y_test = df_test['income'].values.reshape(-1, 1)\n",
    "X_test = df_test.loc[:, ~(df_test.columns).isin(['income'])]\n",
    "\n",
    "# Test the more complex model with larger coefficients.\n",
    "df_test['balance_student'] = df_test['balance'] * df_test['student']\n",
    "df_test['balance_default'] = df_test['balance'] * df_test['default']\n",
    "df_test['student_default'] = df_test['student'] * df_test['default']\n",
    "df_test['balance_sqrt'] = (df_test['balance'] + 100) ** .5\n",
    "df_test['balance2'] = (df_test['balance'] + 100) ** 2\n",
    "df_test['balance3'] = (df_test['balance'] + 100) ** 3\n",
    "X_test2 = df_test.loc[:, ~(df_test.columns).isin(['income'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² for the model with few features:\n",
      "0.36933179155447127\n",
      "\n",
      "Parameter estimates for the model with few features:\n",
      "[-0.         -0.30593479 -0.          0.00185258]\n",
      "\n",
      "Parameter Estimates for the same predictors for the small modeland large model:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-55d7818125cc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m print('\\nParameter Estimates for the same predictors for the small model'\n\u001b[1;32m     12\u001b[0m       'and large model:')\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mcompare\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morigparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m prettycompare = np.array2string(\n\u001b[1;32m     15\u001b[0m     \u001b[0mcompare\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/shape_base.py\u001b[0m in \u001b[0;36mcolumn_stack\u001b[0;34m(tup)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0marrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_nx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input array dimensions except for the concatenation axis must match exactly"
     ]
    }
   ],
   "source": [
    "# Small number of parameters.\n",
    "lass = linear_model.Lasso(alpha=.45)\n",
    "lassfit = lass.fit(X_train, Y_train)\n",
    "print('R² for the model with few features:')\n",
    "print(lass.score(X_train, Y_train))\n",
    "origparams = np.append(lassfit.coef_, lassfit.intercept_)\n",
    "print('\\nParameter estimates for the model with few features:')\n",
    "print(origparams)\n",
    "\n",
    "\n",
    "print('\\nParameter Estimates for the same predictors for the small model'\n",
    "      'and large model:')\n",
    "compare = np.column_stack((origparams, newparams))\n",
    "prettycompare = np.array2string(\n",
    "    compare,\n",
    "    formatter={'float_kind':'{0:.3f}'.format})\n",
    "print(prettycompare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5738739164402877\n",
      "[ 1.36988466e-02 -7.57859433e-01 -3.25298557e-04]\n",
      "\n",
      "Parameter Estimates for the same predictors for the small modeland large model:\n",
      "[[0.014 -0.002]\n",
      " [-0.758 -0.757]\n",
      " [-0.000 0.048]]\n"
     ]
    }
   ],
   "source": [
    "ridgeregr = linear_model.Ridge(alpha=10, fit_intercept=False) \n",
    "ridgeregr.fit(X_train, Y_train)\n",
    "print(ridgeregr.score(X_train, Y_train))\n",
    "origparams = ridgeregr.coef_[0]\n",
    "print(origparams)\n",
    "\n",
    "print('\\nParameter Estimates for the same predictors for the small model'\n",
    "      'and large model:')\n",
    "compare = np.column_stack((origparams, newparams))\n",
    "prettycompare = np.array2string(\n",
    "    compare,\n",
    "    formatter={'float_kind':'{0:.3f}'.format})\n",
    "print(prettycompare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3661388243514323\n",
      "0.4380466345914476\n"
     ]
    }
   ],
   "source": [
    "print(lass.score(X_test, Y_test))\n",
    "\n",
    "print(lassBig.score(X_test2, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = df_train.loc[:, ~(df_train.columns).isin(['income'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression and L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R-squared simple model:\n",
      "0.573878496271703\n",
      "\n",
      "R-squared complex model:\n",
      "0.5739734452073215\n",
      "\n",
      "Parameter Estimates for the same predictors for the small model and large model:\n",
      "[[0.014 -0.004]\n",
      " [-0.759 -0.759]\n",
      " [0.000 4157.868]\n",
      " [-0.001 553434.766]]\n"
     ]
    }
   ],
   "source": [
    "# Load air quality data, drop the index column and any missing data columns.\n",
    "df = pd.read_csv(\n",
    "    'https://vincentarelbundock.github.io/Rdatasets/csv/ISLR/Default.csv'\n",
    ").iloc[:, 1:].dropna()\n",
    "\n",
    "# Recode strings to numeric.\n",
    "df['default'] = np.where(df['default'] == 'Yes', 1, 0)\n",
    "df['student'] = np.where(df['student'] == 'Yes', 1, 0)\n",
    "names = df.columns\n",
    "df = pd.DataFrame(preprocessing.scale(df), columns=names)\n",
    "\n",
    "# Define the training and test sizes.\n",
    "trainsize = int(df.shape[0] / 2)\n",
    "df_test = df.iloc[trainsize:, :].copy()\n",
    "df_train = df.iloc[:trainsize, :].copy()\n",
    "\n",
    "# Set up the regression model to predict defaults using all other\n",
    "# variables as features.\n",
    "regr1 = linear_model.LinearRegression()\n",
    "Y_train = df_train['income'].values.reshape(-1, 1)\n",
    "X_train = df_train.loc[:, ~(df_train.columns).isin(['income'])]\n",
    "regr1.fit(X_train, Y_train)\n",
    "print('\\nR-squared simple model:')\n",
    "print(regr1.score(X_train, Y_train))\n",
    "\n",
    "#Store the parameter estimates.\n",
    "origparams = np.append(regr1.coef_, regr1.intercept_)\n",
    "\n",
    "# Make new features to capture potential quadratic and cubic relationships\n",
    "# between the features.\n",
    "df_train['balance_student'] = df_train['balance'] * df_train['student']\n",
    "df_train['balance_default'] = df_train['balance'] * df_train['default']\n",
    "df_train['student_default'] = df_train['student'] * df_train['default']\n",
    "df_train['balance_sqrt'] = (df_train['balance'] + 100) ** .5\n",
    "df_train['balance2'] = (df_train['balance'] + 100) ** 2\n",
    "df_train['balance3'] = (df_train['balance'] + 100) ** 3\n",
    "\n",
    "# Re-run the model with the new features.\n",
    "regrBig = linear_model.LinearRegression()\n",
    "X_train2 = df_train.loc[:, ~(df_train.columns).isin(['income'])]\n",
    "regrBig.fit(X_train2, Y_train)\n",
    "print('\\nR-squared complex model:')\n",
    "print(regrBig.score(X_train2, Y_train))\n",
    "\n",
    "# Store the new parameter estimates for the same features.\n",
    "newparams = np.append(\n",
    "    regrBig.coef_[0,0:(len(origparams)-1)],\n",
    "    regrBig.intercept_)\n",
    "\n",
    "print('\\nParameter Estimates for the same predictors for the small model '\n",
    "      'and large model:')\n",
    "compare = np.column_stack((origparams, newparams))\n",
    "prettycompare = np.array2string(\n",
    "    compare,\n",
    "    formatter={'float_kind':'{0:.3f}'.format})\n",
    "print(prettycompare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the above model to the test set\n",
    "Look at that intercept (last line)! The R-squared value barely increased, but even so the inflation of the parameters suggests that the gain is due to overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "R-squared simple model:\n",
      "0.5630697922503317\n",
      "\n",
      "R-squared complex model:\n",
      "0.5630239527360242\n"
     ]
    }
   ],
   "source": [
    "# Test the simpler model with smaller coefficients.\n",
    "Y_test = df_test['income'].values.reshape(-1, 1)\n",
    "X_test = df_test.loc[:, ~(df_test.columns).isin(['income'])]\n",
    "print('\\nR-squared simple model:')\n",
    "print(regr1.score(X_test, Y_test))\n",
    "\n",
    "# Test the more complex model with larger coefficients.\n",
    "df_test['balance_student'] = df_test['balance'] * df_test['student']\n",
    "df_test['balance_default'] = df_test['balance'] * df_test['default']\n",
    "df_test['student_default'] = df_test['student'] * df_test['default']\n",
    "df_test['balance_sqrt'] = (df_test['balance'] + 100) ** .5\n",
    "df_test['balance2'] = (df_test['balance'] + 100) ** 2\n",
    "df_test['balance3'] = (df_test['balance'] + 100) ** 3\n",
    "\n",
    "# Re-run the model with the new features.\n",
    "X_test2 = df_test.loc[:, ~(df_test.columns).isin(['income'])]\n",
    "print('\\nR-squared complex model:')\n",
    "print(regrBig.score(X_test2, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yup- the more complex model actually fits worse.  The differences here are quite small, but that isn't always the case.  What happens if we apply ridge regression to this situation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5738739164402877\n",
      "[ 1.36988466e-02 -7.57859433e-01 -3.25298557e-04]\n",
      "0.5739464289613578\n",
      "\n",
      "Parameter Estimates for the same predictors for the small modeland large model:\n",
      "[[0.014 -0.002]\n",
      " [-0.758 -0.757]\n",
      " [-0.000 0.048]]\n"
     ]
    }
   ],
   "source": [
    "# Fitting a ridge regression model. Alpha is the regularization\n",
    "# parameter (usually called lambda). As alpha gets larger, parameter\n",
    "# shrinkage grows more pronounced. Note that by convention, the\n",
    "# intercept is not regularized. Since we standardized the data\n",
    "# earlier, the intercept should be equal to zero and can be dropped.\n",
    "\n",
    "ridgeregr = linear_model.Ridge(alpha=10, fit_intercept=False) \n",
    "ridgeregr.fit(X_train, Y_train)\n",
    "print(ridgeregr.score(X_train, Y_train))\n",
    "origparams = ridgeregr.coef_[0]\n",
    "print(origparams)\n",
    "\n",
    "\n",
    "\n",
    "print('\\nParameter Estimates for the same predictors for the small model'\n",
    "      'and large model:')\n",
    "compare = np.column_stack((origparams, newparams))\n",
    "prettycompare = np.array2string(\n",
    "    compare,\n",
    "    formatter={'float_kind':'{0:.3f}'.format})\n",
    "print(prettycompare)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in magnitude for parameters in the training set is much smaller – no parameter explosion here. Let's check the implications for the fit of the models on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5631088763076704\n",
      "0.5631804323930323\n"
     ]
    }
   ],
   "source": [
    "print(ridgeregr.score(X_test, Y_test))\n",
    "print(ridgeregrBig.score(X_test2, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
